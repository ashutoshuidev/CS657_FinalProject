{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "import seaborn as sns\n",
    "from pyspark.sql.functions import col, mean, monotonically_increasing_id, floor\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier,RandomForestClassifier, GBTClassifier, NaiveBayes, LinearSVC\n",
    "from pyspark.ml.feature import IndexToString,StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8187 525\n",
      "2726 178\n",
      "2738 166\n",
      "2723 181\n",
      "+---------+--------------+-----+------+--------------------+---------+\n",
      "|signal_id|id_measurement|phase|target|            features|signal_id|\n",
      "+---------+--------------+-----+------+--------------------+---------+\n",
      "|        0|             0|    0|     0|[10.809125,0.5727...|        0|\n",
      "|        1|             0|    1|     0|[6.6541875,15.337...|        1|\n",
      "|        2|             0|    2|     0|[-18.8354625,-17....|        2|\n",
      "|        3|             1|    0|     1|[-9.6913125,1.001...|        3|\n",
      "|        4|             1|    1|     1|[-9.7676625,-16.9...|        4|\n",
      "|        5|             1|    2|     1|[18.48585,15.1597...|        5|\n",
      "|        6|             2|    0|     0|[-19.958425,-19.1...|        6|\n",
      "|        7|             2|    1|     0|[12.9744125,2.229...|        7|\n",
      "|        8|             2|    2|     0|[6.595825,16.1692...|        8|\n",
      "|        9|             3|    0|     0|[-8.734875,5.6185...|        9|\n",
      "|       10|             3|    1|     0|[-13.418275,-20.3...|       10|\n",
      "|       11|             3|    2|     0|[20.193325,13.200...|       11|\n",
      "|       12|             4|    0|     0|[6.224725,14.8163...|       12|\n",
      "|       13|             4|    1|     0|[-18.17825,-16.41...|       13|\n",
      "|       14|             4|    2|     0|[10.9962625,0.774...|       14|\n",
      "|       15|             5|    0|     0|[-11.940825,-19.1...|       15|\n",
      "|       16|             5|    1|     0|[16.57915,12.3528...|       16|\n",
      "|       17|             5|    2|     0|[-6.1508,5.39575,...|       17|\n",
      "|       18|             6|    0|     0|[15.329475,7.5545...|       18|\n",
      "|       19|             6|    1|     0|[-1.6246375,10.01...|       19|\n",
      "+---------+--------------+-----+------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+------------+--------------------+\n",
      "|prediction|indexedLabel|            features|\n",
      "+----------+------------+--------------------+\n",
      "|       0.0|         0.0|[10.809125,0.5727...|\n",
      "|       0.0|         1.0|[-9.7676625,-16.9...|\n",
      "|       0.0|         0.0|[6.595825,16.1692...|\n",
      "|       1.0|         0.0|[20.193325,13.200...|\n",
      "|       0.0|         0.0|[6.224725,14.8163...|\n",
      "+----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "0.7222222222222222\n",
      "Test Error = 0.277778 \n",
      "+--------------+------+--------------------+\n",
      "|predictedLabel|target|            features|\n",
      "+--------------+------+--------------------+\n",
      "|             0|     0|[10.809125,0.5727...|\n",
      "|             0|     1|[-9.7676625,-16.9...|\n",
      "|             0|     0|[6.595825,16.1692...|\n",
      "|             1|     0|[20.193325,13.200...|\n",
      "|             0|     0|[6.224725,14.8163...|\n",
      "+--------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "0.7777777777777778\n",
      "Test Error = 0.222222\n",
      "+----------+------------+--------------------+\n",
      "|prediction|indexedLabel|            features|\n",
      "+----------+------------+--------------------+\n",
      "|       0.0|         0.0|[10.809125,0.5727...|\n",
      "|       0.0|         1.0|[-9.7676625,-16.9...|\n",
      "|       0.0|         0.0|[6.595825,16.1692...|\n",
      "|       1.0|         0.0|[20.193325,13.200...|\n",
      "|       0.0|         0.0|[6.224725,14.8163...|\n",
      "+----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "0.7222222222222222\n",
      "Test Error = 0.277778\n",
      "+--------------------+------+----------+\n",
      "|            features|target|prediction|\n",
      "+--------------------+------+----------+\n",
      "|[10.809125,0.5727...|     0|       0.0|\n",
      "|[-9.7676625,-16.9...|     1|       0.0|\n",
      "|[6.595825,16.1692...|     0|       0.0|\n",
      "|[20.193325,13.200...|     0|       0.0|\n",
      "|[6.224725,14.8163...|     0|       0.0|\n",
      "|[-11.940825,-19.1...|     0|       0.0|\n",
      "|[15.329475,7.5545...|     0|       0.0|\n",
      "|[21.5269375,15.95...|     0|       0.0|\n",
      "|[-10.3004625,3.68...|     0|       0.0|\n",
      "|[-5.0177,-13.7787...|     0|       0.0|\n",
      "+--------------------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Test accuracy = 0.944444\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o8937.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 183.0 failed 1 times, most recent failure: Lost task 0.0 in stage 183.0 (TID 358) (USC-W-91BW1X2.ams.com executor driver): org.apache.spark.SparkException: Failed to execute user defined function(NaiveBayes$$Lambda$4544/1283104823: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:1193)\r\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.update(Summarizer.scala:374)\r\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.update(Summarizer.scala:344)\r\n\tat org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.update(interfaces.scala:562)\r\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$1.$anonfun$applyOrElse$2(AggregationIterator.scala:196)\r\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$1.$anonfun$applyOrElse$2$adapted(AggregationIterator.scala:196)\r\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7(AggregationIterator.scala:213)\r\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7$adapted(AggregationIterator.scala:207)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:158)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:77)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:107)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:85)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.lang.IllegalArgumentException: requirement failed: Naive Bayes requires nonnegative feature values but found [-18.8354625,-17.508525,-7.59025,3.3261625,12.7348125,18.43355,17.6959625,7.537125,-3.4242,-12.804725].\r\n\tat scala.Predef$.require(Predef.scala:281)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$.requireNonnegativeValues(NaiveBayes.scala:358)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.$anonfun$trainDiscreteImpl$1(NaiveBayes.scala:177)\r\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUDF.$anonfun$f$2(ScalaUDF.scala:205)\r\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:1190)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2965)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:2965)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.trainDiscreteImpl(NaiveBayes.scala:193)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.$anonfun$trainWithLabelCheck$1(NaiveBayes.scala:160)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.trainWithLabelCheck(NaiveBayes.scala:144)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:133)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:95)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\r\n\tat sun.reflect.GeneratedMethodAccessor223.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(NaiveBayes$$Lambda$4544/1283104823: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:1193)\r\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.update(Summarizer.scala:374)\r\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.update(Summarizer.scala:344)\r\n\tat org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.update(interfaces.scala:562)\r\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$1.$anonfun$applyOrElse$2(AggregationIterator.scala:196)\r\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$1.$anonfun$applyOrElse$2$adapted(AggregationIterator.scala:196)\r\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7(AggregationIterator.scala:213)\r\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7$adapted(AggregationIterator.scala:207)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:158)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:77)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:107)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:85)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\nCaused by: java.lang.IllegalArgumentException: requirement failed: Naive Bayes requires nonnegative feature values but found [-18.8354625,-17.508525,-7.59025,3.3261625,12.7348125,18.43355,17.6959625,7.537125,-3.4242,-12.804725].\r\n\tat scala.Predef$.require(Predef.scala:281)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$.requireNonnegativeValues(NaiveBayes.scala:358)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.$anonfun$trainDiscreteImpl$1(NaiveBayes.scala:177)\r\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUDF.$anonfun$f$2(ScalaUDF.scala:205)\r\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:1190)\r\n\t... 29 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-2666ab9809ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;31m# train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[1;31m# select example rows to display.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spark\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mc:\\spark\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spark\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \"\"\"\n\u001b[0;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o8937.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 183.0 failed 1 times, most recent failure: Lost task 0.0 in stage 183.0 (TID 358) (USC-W-91BW1X2.ams.com executor driver): org.apache.spark.SparkException: Failed to execute user defined function(NaiveBayes$$Lambda$4544/1283104823: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:1193)\r\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.update(Summarizer.scala:374)\r\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.update(Summarizer.scala:344)\r\n\tat org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.update(interfaces.scala:562)\r\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$1.$anonfun$applyOrElse$2(AggregationIterator.scala:196)\r\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$1.$anonfun$applyOrElse$2$adapted(AggregationIterator.scala:196)\r\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7(AggregationIterator.scala:213)\r\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7$adapted(AggregationIterator.scala:207)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:158)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:77)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:107)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:85)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.lang.IllegalArgumentException: requirement failed: Naive Bayes requires nonnegative feature values but found [-18.8354625,-17.508525,-7.59025,3.3261625,12.7348125,18.43355,17.6959625,7.537125,-3.4242,-12.804725].\r\n\tat scala.Predef$.require(Predef.scala:281)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$.requireNonnegativeValues(NaiveBayes.scala:358)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.$anonfun$trainDiscreteImpl$1(NaiveBayes.scala:177)\r\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUDF.$anonfun$f$2(ScalaUDF.scala:205)\r\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:1190)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2965)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:2965)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.trainDiscreteImpl(NaiveBayes.scala:193)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.$anonfun$trainWithLabelCheck$1(NaiveBayes.scala:160)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.trainWithLabelCheck(NaiveBayes.scala:144)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:133)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:95)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\r\n\tat sun.reflect.GeneratedMethodAccessor223.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(NaiveBayes$$Lambda$4544/1283104823: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:1193)\r\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.update(Summarizer.scala:374)\r\n\tat org.apache.spark.ml.stat.SummaryBuilderImpl$MetricsAggregate.update(Summarizer.scala:344)\r\n\tat org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.update(interfaces.scala:562)\r\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$1.$anonfun$applyOrElse$2(AggregationIterator.scala:196)\r\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$1.$anonfun$applyOrElse$2$adapted(AggregationIterator.scala:196)\r\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7(AggregationIterator.scala:213)\r\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7$adapted(AggregationIterator.scala:207)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:158)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:77)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:107)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:85)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\nCaused by: java.lang.IllegalArgumentException: requirement failed: Naive Bayes requires nonnegative feature values but found [-18.8354625,-17.508525,-7.59025,3.3261625,12.7348125,18.43355,17.6959625,7.537125,-3.4242,-12.804725].\r\n\tat scala.Predef$.require(Predef.scala:281)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$.requireNonnegativeValues(NaiveBayes.scala:358)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.$anonfun$trainDiscreteImpl$1(NaiveBayes.scala:177)\r\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUDF.$anonfun$f$2(ScalaUDF.scala:205)\r\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:1190)\r\n\t... 29 more\r\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Create a SparkSession (Note, the config section is only for Windows!)\n",
    "    spark = SparkSession.builder.master('local[*]').config('spark.executor.memory', '12g').config('spark.driver.memory', '12g').config('spark.driver.maxResultSize', '12g').config(\"spark.cores.max\", \"6\").appName(\"FaultDetection\").getOrCreate()\n",
    "    #spark = SparkSession.builder.appName(\"RecommenderSystem\").getOrCreate()\n",
    "    \n",
    "    # Load up data as dataframe\n",
    "    data = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"C:/My_Data/MS/CS657/Project/InputData/metadata_train.csv\")\n",
    "    data.limit(500)\n",
    "    \n",
    "    signalData = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").parquet(\"C:/My_Data/MS/CS657/Project/InputData/train.parquet\")\n",
    "    #signalData = signalData.limit(500)\n",
    "    #signalData.show(20)\n",
    "    \n",
    "    ################################# Visualization of train data ###################################################\n",
    "    \n",
    "    notFaulty = data.select('signal_id').where(data.target == 0).count()\n",
    "    faulty = data.select('signal_id').where(data.target == 1).count()\n",
    "    \n",
    "    # 8187 -  signals are not faulty, while 525 are faulty\n",
    "    print(notFaulty, faulty)\n",
    "    \n",
    "    # phase wise distribution of faulty vs not faulty signals\n",
    "    notFaultyPhase0 = data.select('signal_id').where((data.target == 0) & (data.phase == 0)).count()\n",
    "    faultyPhase0 = data.select('signal_id').where((data.target == 1) & (data.phase == 0)).count()\n",
    "    print(notFaultyPhase0, faultyPhase0)\n",
    "    \n",
    "    notFaultyPhase1 = data.select('signal_id').where((data.target == 0) & (data.phase == 1)).count()\n",
    "    faultyPhase1 = data.select('signal_id').where((data.target == 1) & (data.phase == 1)).count()\n",
    "    print(notFaultyPhase1, faultyPhase1)\n",
    "    \n",
    "    notFaultyPhase2 = data.select('signal_id').where((data.target == 0) & (data.phase == 2)).count()\n",
    "    faultyPhase2 = data.select('signal_id').where((data.target == 1) & (data.phase == 2)).count()\n",
    "    print(notFaultyPhase2, faultyPhase2)\n",
    "    \n",
    "    \n",
    "    meta_data =  data.toPandas()\n",
    "    sns.set(style=\"darkgrid\")\n",
    "    sns.countplot(x = 'target',hue = 'phase',data = meta_data)\n",
    "    \n",
    "    #################################################################################################################\n",
    "    \n",
    "    \n",
    "    ################################# Feature Extraction ############################################################\n",
    "\n",
    "#     signalData = signalData.select(signalData.columns[:500])\n",
    "    \n",
    "#     signalData = signalData.withColumn(\"index\", monotonically_increasing_id())\n",
    "#     #signalData.limit(80000)\n",
    "    \n",
    "#     indexes = signalData.select(col(\"index\"))\n",
    "    \n",
    "#     if (indexes.tail(1)[0]['index']-indexes.head()['index'] != 799999):\n",
    "#         print(\"ID assign error\")\n",
    "#         spark.stop()\n",
    "#         exit()\n",
    "       \n",
    "#     n_aggregate_columns = 80000\n",
    "    \n",
    "#     signalData = signalData.withColumn('index', signalData['index']-indexes.head()['index'])\n",
    "#     #signalData.show(5)\n",
    "    \n",
    "#     signalData = signalData.withColumn('index', floor(signalData['index']/n_aggregate_columns)).groupBy('index').avg().orderBy('index')\n",
    "#     signalData.show()\n",
    "#     print((signalData.count(), len(signalData.columns)))\n",
    "#     signalData =  signalData.drop(col(\"avg(index)\"))\n",
    "#     signalDataWithFeatures = spark.createDataFrame(signalData.toPandas().set_index(\"index\").transpose())\n",
    "#     signalDataWithFeatures.show()\n",
    "    \n",
    "#     assembler = VectorAssembler(inputCols=[x for x in signalDataWithFeatures.columns],outputCol=\"features\")\n",
    "\n",
    "#     features = assembler.transform(signalDataWithFeatures)\n",
    "#     features = features.select(\"features\")\n",
    "    \n",
    "#     features = features.withColumn(\"signal_id\", monotonically_increasing_id())\n",
    "#     indexes = features.select(col(\"signal_id\"))\n",
    "    \n",
    "#     features = features.withColumn('signal_id', features['signal_id']-indexes.head()['signal_id'])\n",
    "#     features.show()\n",
    "#     features.write.parquet(\"C:/My_Data/MS/CS657/Project/InputData/features.parquet\")\n",
    "    #################################################################################################################\n",
    "    \n",
    "    ################################# Decision Tree Classifier #######################################################\n",
    "    \n",
    "    featureData = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").parquet(\"C:/My_Data/MS/CS657/Project/InputData/features.parquet\")\n",
    "    finalData = data.join(featureData,data.signal_id ==  featureData.signal_id,\"inner\")\n",
    "    finalData.show()\n",
    "    \n",
    "    # Index labels, adding metadata to the label column.\n",
    "    # Fit on whole dataset to include all labels in index.\n",
    "    labelIndexer = StringIndexer(inputCol=\"target\", outputCol=\"indexedLabel\").fit(finalData)\n",
    "    # Automatically identify categorical features, and index them.\n",
    "    # We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "    featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(finalData)\n",
    "\n",
    "    # Split the data into training and test sets (30% held out for testing)\n",
    "    (trainingData, testData) = finalData.randomSplit([0.7, 0.3])\n",
    "\n",
    "    # Train a DecisionTree model.\n",
    "    dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "\n",
    "    # Chain indexers and tree in a Pipeline\n",
    "    pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n",
    "\n",
    "    # Train model.  This also runs the indexers.\n",
    "    model = pipeline.fit(trainingData)\n",
    "\n",
    "    # Make predictions.\n",
    "    predictions = model.transform(testData)\n",
    "\n",
    "    # Select example rows to display.\n",
    "    predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
    "\n",
    "    # Select (prediction, true label) and compute test error\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(accuracy)\n",
    "    print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "    \n",
    "    \n",
    "    #################################################################################################################\n",
    "    \n",
    "    ################################# Random Forest Classifier #######################################################\n",
    "    \n",
    "    # Train a RandomForest model.\n",
    "    rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)\n",
    "\n",
    "    # Convert indexed labels back to original labels.\n",
    "    labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                                   labels=labelIndexer.labels)\n",
    "\n",
    "    # Chain indexers and forest in a Pipeline\n",
    "    pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n",
    "\n",
    "    # Train model.  This also runs the indexers.\n",
    "    model = pipeline.fit(trainingData)\n",
    "\n",
    "    # Make predictions.\n",
    "    predictions = model.transform(testData)\n",
    "\n",
    "    # Select example rows to display.\n",
    "    predictions.select(\"predictedLabel\", \"target\", \"features\").show(5)\n",
    "\n",
    "    # Select (prediction, true label) and compute test error\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(accuracy)\n",
    "    print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "    \n",
    "    \n",
    "    #################################################################################################################\n",
    "    \n",
    "    ################################# Gradient-boosted tree Classifier #######################################################\n",
    "    \n",
    "    # Train a GBT model.\n",
    "    gbt = GBTClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", maxIter=10)\n",
    "\n",
    "    # Chain indexers and GBT in a Pipeline\n",
    "    pipeline = Pipeline(stages=[labelIndexer, featureIndexer, gbt])\n",
    "\n",
    "    # Train model.  This also runs the indexers.\n",
    "    model = pipeline.fit(trainingData)\n",
    "\n",
    "    # Make predictions.\n",
    "    predictions = model.transform(testData)\n",
    "\n",
    "    # Select example rows to display.\n",
    "    predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
    "\n",
    "    # Select (prediction, true label) and compute test error\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(accuracy)\n",
    "    print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "    \n",
    "    \n",
    "    #################################################################################################################\n",
    "    \n",
    "    ################################# Linear SVM Classifier ###############################################\n",
    "    \n",
    "    lsvc = LinearSVC(maxIter=10, regParam=0.1, labelCol=\"target\")\n",
    "    \n",
    "    # Fit the model\n",
    "    lsvcModel = lsvc.fit(trainingData)\n",
    "    # Compute predictions for test data\n",
    "    predictions = lsvcModel.transform(testData)\n",
    "\n",
    "    # Show the computed predictions and compare with the original labels\n",
    "    predictions.select(\"features\", \"target\", \"prediction\").show(10)\n",
    "\n",
    "    # Define the evaluator method with the corresponding metric and compute the classification error on test data\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"target\").setMetricName('accuracy')\n",
    "    accuracy = evaluator.evaluate(predictions) \n",
    "\n",
    "    # Show the accuracy\n",
    "    print(\"Test accuracy = %g\" % (accuracy))\n",
    "    \n",
    "    \n",
    "    #################################################################################################################\n",
    "    \n",
    "    \n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
