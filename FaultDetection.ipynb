{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashutosh.bansal\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\ashutosh.bansal\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "C:\\Users\\ashutosh.bansal\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "import seaborn as sns\n",
    "from pyspark.sql.functions import col, mean, monotonically_increasing_id, floor\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-----+------+\n",
      "|signal_id|id_measurement|phase|target|\n",
      "+---------+--------------+-----+------+\n",
      "|        0|             0|    0|     0|\n",
      "|        1|             0|    1|     0|\n",
      "|        2|             0|    2|     0|\n",
      "|        3|             1|    0|     1|\n",
      "|        4|             1|    1|     1|\n",
      "|        5|             1|    2|     1|\n",
      "|        6|             2|    0|     0|\n",
      "|        7|             2|    1|     0|\n",
      "|        8|             2|    2|     0|\n",
      "|        9|             3|    0|     0|\n",
      "|       10|             3|    1|     0|\n",
      "|       11|             3|    2|     0|\n",
      "|       12|             4|    0|     0|\n",
      "|       13|             4|    1|     0|\n",
      "|       14|             4|    2|     0|\n",
      "|       15|             5|    0|     0|\n",
      "|       16|             5|    1|     0|\n",
      "|       17|             5|    2|     0|\n",
      "|       18|             6|    0|     0|\n",
      "|       19|             6|    1|     0|\n",
      "+---------+--------------+-----+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "8187 525\n",
      "2726 178\n",
      "2738 166\n",
      "2723 181\n",
      "(10, 4002)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Create a SparkSession (Note, the config section is only for Windows!)\n",
    "    spark = SparkSession.builder.master('local[*]').config('spark.executor.memory', '12g').config('spark.driver.memory', '12g').config('spark.driver.maxResultSize', '12g').config(\"spark.cores.max\", \"6\").appName(\"FaultDetection\").getOrCreate()\n",
    "    #spark = SparkSession.builder.appName(\"RecommenderSystem\").getOrCreate()\n",
    "    \n",
    "    # Load up data as dataframe\n",
    "    data = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"C:/My_Data/MS/CS657/Project/InputData/metadata_train.csv\")\n",
    "    data.show(20)\n",
    "    \n",
    "    signalData = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").parquet(\"C:/My_Data/MS/CS657/Project/InputData/train.parquet\")\n",
    "    #signalData = signalData.limit(500)\n",
    "    #signalData.show(20)\n",
    "    \n",
    "    ################################# Visualization of train data ###################################################\n",
    "    \n",
    "    notFaulty = data.select('signal_id').where(data.target == 0).count()\n",
    "    faulty = data.select('signal_id').where(data.target == 1).count()\n",
    "    \n",
    "    # 8187 -  signals are not faulty, while 525 are faulty\n",
    "    print(notFaulty, faulty)\n",
    "    \n",
    "    # phase wise distribution of faulty vs not faulty signals\n",
    "    notFaultyPhase0 = data.select('signal_id').where((data.target == 0) & (data.phase == 0)).count()\n",
    "    faultyPhase0 = data.select('signal_id').where((data.target == 1) & (data.phase == 0)).count()\n",
    "    print(notFaultyPhase0, faultyPhase0)\n",
    "    \n",
    "    notFaultyPhase1 = data.select('signal_id').where((data.target == 0) & (data.phase == 1)).count()\n",
    "    faultyPhase1 = data.select('signal_id').where((data.target == 1) & (data.phase == 1)).count()\n",
    "    print(notFaultyPhase1, faultyPhase1)\n",
    "    \n",
    "    notFaultyPhase2 = data.select('signal_id').where((data.target == 0) & (data.phase == 2)).count()\n",
    "    faultyPhase2 = data.select('signal_id').where((data.target == 1) & (data.phase == 2)).count()\n",
    "    print(notFaultyPhase2, faultyPhase2)\n",
    "    \n",
    "    \n",
    "    meta_data =  data.toPandas()\n",
    "    sns.set(style=\"darkgrid\")\n",
    "    sns.countplot(x = 'target',hue = 'phase',data = meta_data)\n",
    "    \n",
    "    #################################################################################################################\n",
    "    \n",
    "    \n",
    "    ################################# Feature Extraction ############################################################\n",
    "\n",
    "    signalData = signalData.select(signalData.columns[:4000])\n",
    "    \n",
    "    signalData = signalData.withColumn(\"index\", monotonically_increasing_id())\n",
    "    #signalData.limit(80000)\n",
    "    \n",
    "    indexes = signalData.select(col(\"index\"))\n",
    "    \n",
    "    if (indexes.tail(1)[0]['index']-indexes.head()['index'] != 799999):\n",
    "        print(\"ID assign error\")\n",
    "        spark.stop()\n",
    "        exit()\n",
    "       \n",
    "    n_aggregate_columns = 80000\n",
    "    \n",
    "    signalData = signalData.withColumn('index', signalData['index']-indexes.head()['index'])\n",
    "    #signalData.show(5)\n",
    "    \n",
    "    signalData = signalData.withColumn('index', floor(signalData['index']/n_aggregate_columns)).groupBy('index').avg().orderBy('index')\n",
    "    #signalData.show()\n",
    "    print((signalData.count(), len(signalData.columns)))\n",
    "    signalData =  signalData.drop(col(\"avg(index)\"))\n",
    "    signalDataWithFeatures = spark.createDataFrame(signalData.toPandas().set_index(\"index\").transpose())\n",
    "    #signalDataWithFeatures.show()\n",
    "    \n",
    "    assembler = VectorAssembler(inputCols=[x for x in signalDataWithFeatures.columns],outputCol=\"features\")\n",
    "\n",
    "    features = assembler.transform(signalDataWithFeatures)\n",
    "    features = features.select(\"features\")\n",
    "    \n",
    "    features = features.withColumn(\"signal_id\", monotonically_increasing_id())\n",
    "    indexes = features.select(col(\"signal_id\"))\n",
    "    \n",
    "    features = features.withColumn('signal_id', features['signal_id']-indexes.head()['signal_id'])\n",
    "    features.show()\n",
    "    #################################################################################################################\n",
    "    \n",
    "    \n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
