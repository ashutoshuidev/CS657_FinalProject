{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashutosh.bansal\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\ashutosh.bansal\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "C:\\Users\\ashutosh.bansal\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "import seaborn as sns\n",
    "from pyspark.sql.functions import col, mean, monotonically_increasing_id, floor\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import VectorUDT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 500\n",
      "DataFrame[index: bigint]\n",
      "(10, 502)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Create a SparkSession (Note, the config section is only for Windows!)\n",
    "    spark = SparkSession.builder.master('local[*]').config('spark.executor.memory', '12g').config('spark.driver.memory', '12g').config('spark.driver.maxResultSize', '12g').config(\"spark.cores.max\", \"6\").appName(\"FaultDetection\").getOrCreate()\n",
    "    #spark = SparkSession.builder.appName(\"RecommenderSystem\").getOrCreate()\n",
    "    \n",
    "    # Load up data as dataframe\n",
    "    #data = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"C:/My_Data/MS/CS657/Project/InputData/metadata_train.csv\")\n",
    "    #data.limit(500)\n",
    "    \n",
    "    #signalData = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").parquet(\"C:/My_Data/MS/CS657/Project/InputData/train.parquet\")\n",
    "    \n",
    "    \n",
    "    ################################# Visualization of train data ###################################################\n",
    "    \n",
    "#     notFaulty = data.select('signal_id').where(data.target == 0).count()\n",
    "#     faulty = data.select('signal_id').where(data.target == 1).count()\n",
    "    \n",
    "#     # 8187 -  signals are not faulty, while 525 are faulty\n",
    "#     print(notFaulty, faulty)\n",
    "    \n",
    "#     # phase wise distribution of faulty vs not faulty signals\n",
    "#     notFaultyPhase0 = data.select('signal_id').where((data.target == 0) & (data.phase == 0)).count()\n",
    "#     faultyPhase0 = data.select('signal_id').where((data.target == 1) & (data.phase == 0)).count()\n",
    "#     print(notFaultyPhase0, faultyPhase0)\n",
    "    \n",
    "#     notFaultyPhase1 = data.select('signal_id').where((data.target == 0) & (data.phase == 1)).count()\n",
    "#     faultyPhase1 = data.select('signal_id').where((data.target == 1) & (data.phase == 1)).count()\n",
    "#     print(notFaultyPhase1, faultyPhase1)\n",
    "    \n",
    "#     notFaultyPhase2 = data.select('signal_id').where((data.target == 0) & (data.phase == 2)).count()\n",
    "#     faultyPhase2 = data.select('signal_id').where((data.target == 1) & (data.phase == 2)).count()\n",
    "#     print(notFaultyPhase2, faultyPhase2)\n",
    "    \n",
    "    \n",
    "#     meta_data =  data.toPandas()\n",
    "#     sns.set(style=\"darkgrid\")\n",
    "#     sns.countplot(x = 'target',hue = 'phase',data = meta_data)\n",
    "    \n",
    "    #################################################################################################################\n",
    "    \n",
    "    \n",
    "    ################################# Feature Extraction ############################################################\n",
    "\n",
    "    # Create an empty dataframe with a schema\n",
    "    schema = StructType([StructField('features', VectorUDT(), True)])\n",
    "    finalfeatures = spark.createDataFrame([], schema)\n",
    "\n",
    "    for i in range(0,20):\n",
    "        signalData = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").parquet(\"C:/My_Data/MS/CS657/Project/InputData/train.parquet\")\n",
    "        signalData = signalData.select(signalData.columns[500*i:500*(i+1)])\n",
    "        print(500*i,500*(i+1) )\n",
    "        signalData = signalData.withColumn(\"index\", monotonically_increasing_id())\n",
    "        \n",
    "        indexes = signalData.select(col(\"index\"))\n",
    "        print(indexes)\n",
    "        minindex = indexes.head()['index']\n",
    "        if (indexes.tail(1)[0]['index']-minindex != 799999):\n",
    "            print(\"ID assign error\")\n",
    "            spark.stop()\n",
    "            exit()\n",
    "\n",
    "        n_aggregate_columns = 80000\n",
    "        signalData = signalData.withColumn('index', signalData['index']-minindex)\n",
    "        signalData = signalData.withColumn('index', floor(signalData['index']/n_aggregate_columns)).groupBy('index').avg().orderBy('index')\n",
    "        print((signalData.count(), len(signalData.columns)))\n",
    "        signalData =  signalData.drop(col(\"avg(index)\"))\n",
    "        signalDataWithFeatures = spark.createDataFrame(signalData.toPandas().set_index(\"index\").transpose())\n",
    "        assembler = VectorAssembler(inputCols=[x for x in signalDataWithFeatures.columns],outputCol=\"features\")\n",
    "\n",
    "        features = assembler.transform(signalDataWithFeatures)\n",
    "        features = features.select(\"features\")\n",
    "\n",
    "        features.show()\n",
    "        finalfeatures = finalfeatures.union(features)\n",
    "    \n",
    "    finalfeatures = finalfeatures.withColumn(\"signal_id\", monotonically_increasing_id())\n",
    "    indexes = finalfeatures.select(col(\"signal_id\"))\n",
    "    \n",
    "    finalfeatures = finalfeatures.withColumn('signal_id', finalfeatures['signal_id']-indexes.head()['signal_id'])\n",
    "    finalfeatures.write.parquet(\"C:/My_Data/MS/CS657/Project/InputData/featuresData/finalfeatures.parquet\")\n",
    "    #################################################################################################################\n",
    "    \n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
